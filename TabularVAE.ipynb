{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "I4PczoTMNXi3",
    "outputId": "c9842093-2082-44ee-fac6-c191946c1e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastai\n",
      "Version: 2.0.15\n",
      "Summary: fastai simplifies training fast and accurate neural nets using modern best practices\n",
      "Home-page: https://github.com/fastai/fastai/tree/master/\n",
      "Author: Jeremy Howard, Sylvain Gugger, and contributors\n",
      "Author-email: info@fast.ai\n",
      "License: Apache Software License 2.0\n",
      "Location: /usr/local/lib/python3.6/dist-packages\n",
      "Requires: torchvision, scikit-learn, packaging, pip, pyyaml, pillow, torch, fastprogress, requests, spacy, scipy, pandas, fastcore, matplotlib\n",
      "Required-by: \n",
      "---\n",
      "Name: fastcore\n",
      "Version: 1.0.16\n",
      "Summary: Python supercharged for fastai development\n",
      "Home-page: https://github.com/fastai/fastcore/tree/master/\n",
      "Author: Jeremy Howard and Sylvain Gugger\n",
      "Author-email: infos@fast.ai\n",
      "License: Apache Software License 2.0\n",
      "Location: /usr/local/lib/python3.6/dist-packages\n",
      "Requires: packaging, pip\n",
      "Required-by: fastai\n"
     ]
    }
   ],
   "source": [
    "!pip show fastai fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pS3C4jJLEdOV"
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu3gjQ7CNMSv"
   },
   "source": [
    "We'll use the `Adult Sample` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UmzTzqPTEttQ"
   },
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bCOcb3DNSVd"
   },
   "source": [
    "And declare the relevent information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2YqxE0AVEdOX"
   },
   "outputs": [],
   "source": [
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "y_names = 'salary'\n",
    "y_block = CategoryBlock()\n",
    "splits = RandomSplitter()(range_of(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUIkMhCENnLd"
   },
   "source": [
    "Next we need our own version of `ReadTabBatch` that will return our inputs\n",
    "\n",
    "> The continous variables are still normalized if we used `Normalize`. Couldn't figure out an easy way to de-norm it, but it's okay that we do not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "08quVRZuEdOc"
   },
   "outputs": [],
   "source": [
    "class ReadTabBatchIdentity(ItemTransform):\n",
    "    \"Read a batch of data and return the inputs as both `x` and `y`\"\n",
    "    def __init__(self, to): store_attr()\n",
    "\n",
    "    def encodes(self, to):\n",
    "        if not to.with_cont: res = (tensor(to.cats).long(),) + (tensor(to.cats).long(),)\n",
    "        else: res = (tensor(to.cats).long(),tensor(to.conts).float()) + (tensor(to.cats).long(), tensor(to.conts).float())\n",
    "        if to.device is not None: res = to_device(res, to.device)\n",
    "        return res\n",
    "    \n",
    "class TabularPandasIdentity(TabularPandas): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itUJ_YaoN6uT"
   },
   "source": [
    "Next we need to make a new `TabDataLoader` that uses our `RadTabBatchIdentity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C0YUhcd3EdOe"
   },
   "outputs": [],
   "source": [
    "@delegates()\n",
    "class TabDataLoaderIdentity(TabDataLoader):\n",
    "    \"A transformed `DataLoader` for AutoEncoder problems with Tabular data\"\n",
    "    do_item = noops\n",
    "    def __init__(self, dataset, bs=16, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n",
    "        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatchIdentity(dataset)\n",
    "        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n",
    "\n",
    "    def create_batch(self, b): return self.dataset.iloc[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX9Vv9OPOAVl"
   },
   "source": [
    "And make `TabularPandasIdentity`'s `dl_type` to `TabDataLoaderIdentity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q5EEsDr-1S5_"
   },
   "outputs": [],
   "source": [
    "TabularPandasIdentity._dl_type = TabDataLoaderIdentity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXmzs8Hx1WQd"
   },
   "source": [
    "To start we'll make a very basic `to` object using our new `TabularPandasIdentity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4sWPDJNy1aK1"
   },
   "outputs": [],
   "source": [
    "bs=1024\n",
    "\n",
    "to = TabularPandasIdentity(df, [Categorify, FillMissing, Normalize], cat_names, cont_names, splits=RandomSplitter(seed=32)(df))\n",
    "dls = to.dataloaders(bs=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kcUFTYoOGO0"
   },
   "source": [
    "Set the `n_inp` to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uQDN7OpH1b9B"
   },
   "outputs": [],
   "source": [
    "dls.n_inp = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojWg3NDI1iXx"
   },
   "source": [
    "And then we'll calculate the embedding sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NERxSEdK1jyv"
   },
   "outputs": [],
   "source": [
    "emb_szs = get_emb_sz(to.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5neKs1q1kuF"
   },
   "source": [
    "For each categorical variable we need to know the total possible values it can have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "Uv7-oVlPEdOg",
    "outputId": "bdfd5add-2f30-423a-aedc-42d564110157"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workclass': 10,\n",
       " 'education': 17,\n",
       " 'marital-status': 8,\n",
       " 'occupation': 16,\n",
       " 'relationship': 7,\n",
       " 'race': 6,\n",
       " 'education-num_na': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cats = {k:len(v) for k,v in to.classes.items()}\n",
    "total_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKNiQtro13Dk"
   },
   "source": [
    "We will need this dictionary in our loss function to figure out where to apply our `CrossEntropyLossFlat` for each categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpyLIJsO2DwA"
   },
   "source": [
    "Next we need to know the total number ouf outputs possible for our categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "u56gAbBYEdOi",
    "outputId": "e8a87efe-6dc2-48c8-87b8-15a2c088922d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v for k,v in total_cats.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BjAP1OVOYsQ"
   },
   "source": [
    "And let's keep a batch of our data for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RAqcnZI2EdOl"
   },
   "outputs": [],
   "source": [
    "batch = dls.one_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5c7Vi-f2aLc"
   },
   "source": [
    "Next we need to know the means and standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "rvbLzGcV2cDP",
    "outputId": "f17df585-8136-4869-ec7c-d263e3052d26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 38.5793696495067,\n",
       " 'fnlwgt': 190006.02011593536,\n",
       " 'education-num': 10.079158782958984}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhHUG2Sk2p42"
   },
   "source": [
    "We can store them in a `DataFrame` for easy adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ttuLY8iWEdOn"
   },
   "outputs": [],
   "source": [
    "means = pd.DataFrame.from_dict({k:[v] for k,v in to.means.items()})\n",
    "stds = pd.DataFrame.from_dict({k:[v] for k,v in to.stds.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5rHaP4N2yIl"
   },
   "source": [
    "We'll also use a SigmoidRange based on the un-normalized data to reduce the range our values can be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lqoao9U5EdOo"
   },
   "outputs": [],
   "source": [
    "low = (df[cont_names].min().to_frame().T.values - means.values) / stds.values\n",
    "high = (df[cont_names].max().to_frame().T.values - means.values) / stds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "2DcGJCze3Kqa",
    "outputId": "77bf0d20-aef1-426b-eeda-7c969c73cac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.57952443, -1.67843578, -3.55622464]]),\n",
       " array([[ 3.76378659, 12.22741736,  2.31914013]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu8pZ9oSEdOq"
   },
   "source": [
    "## Batch Swap Noise\n",
    "Used in the winning solution for the Kaggle competition [Puerto Seguro Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629#250927)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NZb6cetaEdOr"
   },
   "outputs": [],
   "source": [
    "class BatchSwapNoise(Module):\n",
    "    \"Swap Noise Module\"\n",
    "    def __init__(self, p): store_attr()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = torch.rand(x.size()) > (1 - self.p)\n",
    "            l1 = torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor)\n",
    "            l2 = (mask.type(torch.LongTensor) * x.size(1))\n",
    "            res = (l1 * l2).view(-1)\n",
    "            idx = torch.arange(x.nelement()) + res\n",
    "            idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n",
    "            return x.flatten()[idx].view(x.size())\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjduDB3JOpfi"
   },
   "source": [
    "We'll make a custom `TabularVAE` model (Denoising Variational AutoEncoder) for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "0moqukV_EdOs"
   },
   "outputs": [],
   "source": [
    "class TabularVAE(TabularModel):\n",
    "    def __init__(self, emb_szs, n_cont, hidden_size, cats, low, high, ps=0.2, embed_p=0.01, bswap=None, act_cls=Swish()):\n",
    "        super().__init__(emb_szs, n_cont, layers=[1024, 512, 256], out_sz=hidden_size, embed_p=embed_p, act_cls=act_cls)\n",
    "        \n",
    "        self.bswap = bswap\n",
    "        self.cats = cats\n",
    "        self.activation_cats = sum([v for k,v in cats.items()])\n",
    "        \n",
    "        self.layers = nn.Sequential(*L(self.layers.children())[:-1] + nn.Sequential(LinBnDrop(256, hidden_size, p=ps, act=act_cls)))\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_std = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        if self.bswap != None: self.noise = BatchSwapNoise(self.bswap)\n",
    "        self.decoder = nn.Sequential(\n",
    "            LinBnDrop(hidden_size, 256, p=ps, act=act_cls),\n",
    "            LinBnDrop(256, 512, p=ps, act=act_cls),\n",
    "            LinBnDrop(512, 1024, p=ps, act=act_cls)\n",
    "        )\n",
    "        \n",
    "        self.decoder_cont = nn.Sequential(\n",
    "            LinBnDrop(1024, n_cont, p=ps, bn=False, act=None),\n",
    "            SigmoidRange(low=low, high=high)\n",
    "        )\n",
    "        \n",
    "        self.decoder_cat = LinBnDrop(1024, self.activation_cats, p=ps, bn=False, act=None)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.cuda.HalfTensor(*mu.size()).normal_()\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc_mu(h), F.softplus(self.fc_std(h))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def forward(self, x_cat, x_cont=None, encode=False):\n",
    "        if(self.bswap != None):\n",
    "            x_cat = self.noise(x_cat)\n",
    "            x_cont = self.noise(x_cont)\n",
    "        \n",
    "        encoded = super().forward(x_cat, x_cont)\n",
    "        z, mu, logvar = self.bottleneck(encoded)\n",
    "        if(encode): return z\n",
    "        \n",
    "        decoded_trunk = self.decoder(z)\n",
    "        \n",
    "        decoded_cats = self.decoder_cat(decoded_trunk)\n",
    "        \n",
    "        decoded_conts = self.decoder_cont(decoded_trunk)\n",
    "        \n",
    "        return decoded_cats, decoded_conts, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO2gBJNoO3hP"
   },
   "source": [
    "We'll also need a loss function that can grade how well our features represent the original dataset. \n",
    "\n",
    "The categorical features will be graded on `CrossEntropyLossFlat` and the continous with `MSELossFlat`.\n",
    "\n",
    "Since this is a Variationnal AutoEncoder, we have to worry about KL-Divergence too.  kl_weight is a special parameter controlled by a callback.  At the beginning this parameter will be zero (basically like a normal autoencoder) and we will gradually increase it to 1 so that the auto-encoder become variationnal.  This is a trick suggested in [Ladder Variational AutoEncoder](https://arxiv.org/abs/1602.02282) and also used in the [NVAE](https://arxiv.org/abs/2007.03898) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "24aYC4fk4Qev"
   },
   "outputs": [],
   "source": [
    "class VAERecreatedLoss(Module):\n",
    "    \"Measures how well we have created the original tabular inputs, plus the KL Divergence with the unit normal distribution\"\n",
    "    def __init__(self, cat_dict, dataset_size, bs):\n",
    "        ce = CrossEntropyLossFlat(reduction='sum')\n",
    "        mse = MSELossFlat(reduction='sum')\n",
    "        store_attr('cat_dict,ce,mse,dataset_size,bs')\n",
    "        \n",
    "    def forward(self, preds, cat_targs, cont_targs):\n",
    "        if(len(preds) == 5):\n",
    "            cats,conts, mu, logvar, kl_weight = preds\n",
    "        else:\n",
    "            cats,conts, mu, logvar = preds\n",
    "            kl_weight = 1\n",
    "\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        CE = cats.new([0])\n",
    "        pos=0\n",
    "        for i, (k,v) in enumerate(total_cats.items()):\n",
    "            CE += self.ce(cats[:, pos:pos+v], cat_targs[:, i])\n",
    "            pos += v\n",
    "\n",
    "        norm_cats = cats.new([len(total_cats.keys())])\n",
    "        norm_conts = conts.new([conts.size(1)])\n",
    "        total = (self.mse(conts, cont_targs)/norm_conts) + (CE/norm_cats)\n",
    "        \n",
    "        # This factor depends on your batch size and the size of the dataset.  A good rule of thumb is df.shape[0] / batch_size\n",
    "        # if we don't have this, the KLD loss might become much larger than the reconstruction loss\n",
    "        total *= self.dataset_size / self.bs\n",
    "\n",
    "        return (total + (kl_weight * KLD)) / cats.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RV7j8WoePBJO"
   },
   "source": [
    "All we need to do is pass in our `total_cats` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "zbsDGXPX_VUV"
   },
   "outputs": [],
   "source": [
    "loss_func = VAERecreatedLoss(total_cats, df.shape[0], bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some metrics for stuff we care about while fitting the model.  We have reconstruction metrics like MSE and CrossEntropy but we also have to worry about KLD.\n",
    "Those metrics will help us see if the loss is dominated either by the KLD or by the reconstruction loss from MSE and CrossEntropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEMetric(Metric):\n",
    "    def __init__(self): self.preds = []\n",
    "    def accumulate(self, learn):\n",
    "        cats, conts, mu, logvar = learn.pred\n",
    "        cat_targs, cont_targs = learn.y\n",
    "        norm_conts = conts.new([conts.size(1)])\n",
    "        self.preds.append(to_detach(F.mse_loss(conts, cont_targs, reduction='sum') / norm_conts))\n",
    "    @property\n",
    "    def value(self):\n",
    "        return np.array(self.preds).mean()\n",
    "    \n",
    "class CEMetric(Metric):\n",
    "    def __init__(self): self.preds = []\n",
    "    def accumulate(self, learn):\n",
    "        cats, conts, mu, logvar = learn.pred\n",
    "        cat_targs, cont_targs = learn.y\n",
    "        CE = cats.new([0])\n",
    "        pos=0\n",
    "        for i, (k,v) in enumerate(total_cats.items()):\n",
    "            CE += F.cross_entropy(cats[:, pos:pos+v], cat_targs[:, i], reduction='sum')\n",
    "            pos += v\n",
    "\n",
    "        norm = cats.new([len(total_cats.keys())])\n",
    "        self.preds.append(to_detach(CE/norm))\n",
    "    @property\n",
    "    def value(self):\n",
    "        return np.array(self.preds).mean()\n",
    "    \n",
    "class KLDMetric(Metric):\n",
    "    def __init__(self): self.preds = []\n",
    "    def accumulate(self, learn):\n",
    "        cats, conts, mu, logvar = learn.pred\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        self.preds.append(to_detach(KLD))\n",
    "    @property\n",
    "    def value(self):\n",
    "        return np.array(self.preds).mean()\n",
    "    \n",
    "class MUMetric(Metric):\n",
    "    def __init__(self): self.preds = []\n",
    "    def accumulate(self, learn):\n",
    "        cats, conts, mu, logvar = learn.pred\n",
    "        self.preds.append(to_detach(mu.mean()))\n",
    "    @property\n",
    "    def value(self):\n",
    "        return np.array(self.preds).mean()\n",
    "    \n",
    "class StdMetric(Metric):\n",
    "    def __init__(self): self.preds = []\n",
    "    def accumulate(self, learn):\n",
    "        cats, conts, mu, logvar = learn.pred\n",
    "        self.preds.append(to_detach((logvar.exp_() ** .5).mean()))\n",
    "    @property\n",
    "    def value(self):\n",
    "        return np.array(self.preds).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AnnealedLossCallback will inject the variable `kl_weight` in our loss.  You can see the schedule graph of the parameter.  At the beginning it will be 0, thus the KLD part of the loss will get ignored.  So during 10% of training, we will fit a normal auto-encoder.  Then gradually for 30% of trainning, increase `kl_weight` to 1 and then remain there for the remaining training time so that the auto encoder now becomes full variational.  The way this callback is done, the loss will receive this parameter, but not the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f130d21f1f0>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZlElEQVR4nO3de3Bd5X3u8e9PW5LvF2HJF+Q7yNjCYMDCOBDAYAw2nIRwwkmANEzphVIu6TntmUI7pfmDdtpMpz05mZC6LsNQmhwIAwQMMVcbY07B1Db4LtkWNrZlWdryTdiyrMvWr39IIUKWrG157b32Xvv5zGjQ2mtJet6R9bD0aq31mrsjIiLZLy/sACIiEgwVuohIRKjQRUQiQoUuIhIRKnQRkYjID+sLFxcX+9SpU8P68iIiWWnDhg2H3L2kt32hFfrUqVNZv359WF9eRCQrmdnevvZpykVEJCJU6CIiEaFCFxGJCBW6iEhEqNBFRCKi30I3s6fNLG5mW/vYb2b2EzOrNrPNZnZF8DFFRKQ/yZyhPwMsPsP+JUBZ19v9wD+feywRETlb/V6H7u5rzGzqGQ65HXjWO5/Du9bMRpvZBHc/GFRIyR1Hm1r5eM8Rjp1s5YtTbZxoSYAe8SwRUzH1PK6b0eu9QeckiBuLSoH93bZrul47rdDN7H46z+KZPHlyAF9aosDd+eizwzy3bj9vba2jNdHxlf1mIQUTSZEHrr8gYwu9tx+3Xk+p3H0ZsAygoqJCp13CoRMt/OkLm1izs4GRg/O556rJfGPO+YwfNZhRQwoYVhjD1OgiSQmi0GuASd22JwK1AXxeibi1uw/zg+c+5VhzGz/8Rjl3z5vM4IJY2LFEslYQly0uB+7tutplPtCo+XPpz7+v3cs9/7qW4YPyeeXBa7jvmmkqc5Fz1O8Zupk9BywAis2sBvghUADg7kuBFcCtQDVwErgvVWElGl7deIDHX9nKTbPG8uO7Lmf4oNCeEScSKclc5XJ3P/sdeCiwRBJpq3fE+bMXNjF/+nn89J4rdFYuEiDdKSpp8+m+o/zxzz9hxrgRLLu3QmUuEjAVuqRF48k2HvzFJxSPKOSZ37uSkYMLwo4kEjmavJS0ePzVrTQcb+GlP76asSMGhx1HJJJ0hi4p9+rGAyzfVMsPFpYxZ9LosOOIRJYKXVKq9lgzf/XKVi6fPJoHF1wQdhyRSFOhS8q4O3/x8hYSHc6Pv3sZ+TH9cxNJJf2EScq8tyPO+zsb+LObL2LKmGFhxxGJPBW6pERrewd/83ol00uGce/XpoQdRyQnqNAlJZ796HN2H2ri8dvKKdBUi0ha6CdNAnekqZX/u3IX180oYcFFwT8iVER6p0KXwP2fd3ZysjXB47fN0qNvRdJIhS6BOnCsmefX7ePueZMoGzci7DgiOUWFLoFauvozAB5ccGHISURyjwpdAlPXeIpfrtvPnXMncf7oIWHHEck5KnQJzL+s+YyEu+4IFQmJCl0C0XC8hf/38T7uuLyUSecNDTuOSE5SoUsgnvpgN22JDh66QXPnImFRocs5++JUGz9fu5f/dun5TCvWLf4iYVGhyzl7cX0NTa0J/vDa6WFHEclpKnQ5Jx0dzr999DlzpxRxycRRYccRyWkqdDkn7+2Is/fwSe67ZmrYUURyngpdzskzH37O+JGDueXi8WFHEcl5KnQZsF31x/lg1yG+/7UpeqKiSAbQT6EM2DMffk5hfh53z5scdhQRQYUuA3T8VBsvf3KA2+ecz3nDCsOOIyKo0GWAXtt0kOa2BN+br9WIRDKFCl0G5Jfr9jFz/Ajm6FJFkYyhQpezVnnwCzbVNPKdiklawEIkg6jQ5az9ct1+CmN53HF5adhRRKQbFbqclVNtCX716QFumT2eIv0xVCSjqNDlrLy1rY7G5ja+WzEp7Cgi0kNShW5mi81sh5lVm9ljvewfZWavmdkmM9tmZvcFH1UywQvr9zOxaAhXXzAm7Cgi0kO/hW5mMeBJYAlQDtxtZuU9DnsI2O7uc4AFwD+amX4fj5iaoyf5j+rDfKdiEnl5+mOoSKZJ5gx9HlDt7rvdvRV4Hri9xzEOjLDOSx6GA0eA9kCTSuhe3VgLoD+GimSoZAq9FNjfbbum67XufgrMAmqBLcCfuHtHz09kZveb2XozW9/Q0DDAyBIGd+eVTw9QMaVIS8yJZKhkCr233629x/YtwEbgfOAy4KdmNvK0D3Jf5u4V7l5RUlJyllElTNsPfsGu+Am+pbNzkYyVTKHXAN0vaZhI55l4d/cBL3unamAPMDOYiJIJXvn0AAUx47ZLJoQdRUT6kEyhrwPKzGxa1x867wKW9zhmH7AQwMzGARcBu4MMKuFJdDivbqzl+hljde25SAbL7+8Ad283s4eBt4AY8LS7bzOzB7r2LwWeAJ4xsy10TtE86u6HUphb0mjt7sPEj7foj6EiGa7fQgdw9xXAih6vLe32fi1wc7DRJFP86tMDjBiUz8JZY8OOIiJnoDtF5YxOtSV4c2sdi2ePZ3BBLOw4InIGKnQ5o9U74pxoaef2yzTdIpLpVOhyRq9vPsiYYYXMn35e2FFEpB8qdOlTc2uClZVxFs8eT74WgRbJePoplT69tyNOc1uC2y7Vteci2UCFLn369eaDFA8v5KpperKiSDZQoUuvTra2s7KqnsWzxxPTkxVFsoIKXXq1qirOqbYObrvk/LCjiEiSVOjSq87plkHMm6arW0SyhQpdTtPU0s6qqji3XqLpFpFsokKX06ze0UBLewe36smKIllFhS6neXNbHWOGFXLlVE23iGQTFbp8RUt7gveq4iwqH6fpFpEso0KXr/iw+jAnWtq5Zfb4sKOIyFlSoctXvLm1juGD8rn6At1MJJJtVOjypUSH805lPTfOHMugfD0qVyTbqNDlS+s+P8KRplYWa7pFJCup0OVLb26tY1B+HtfPKAk7iogMgApdAHB33t5Wx7VlJQwblNTKhCKSYVToAsCWA43UNp7ilovHhR1FRAZIhS4AvLO9nliecdMsFbpItlKhC9BZ6BVTiigaVhh2FBEZIBW6sP/ISarqjrOoXGfnItlMhS68s70eQIUukuVU6MI72+spGzucKWOGhR1FRM6BCj3HNZ5s4z8/P6Kzc5EIUKHnuPd2xEl0uApdJAJU6Dnune31lIwYxJyJo8OOIiLnSIWew1raE7y/s4GbZo0lT88+F8l6KvQctnb3EU60tOtmIpGISKrQzWyxme0ws2oze6yPYxaY2UYz22Zm7wcbU1JhZWU9gwvyuObC4rCjiEgA+n0Kk5nFgCeBRUANsM7Mlrv79m7HjAZ+Bix2931mNjZFeSUg7s7Kyjhfv7CYwQV69rlIFCRzhj4PqHb33e7eCjwP3N7jmHuAl919H4C7x4ONKUHbUX+cA8eaWajpFpHISKbQS4H93bZrul7rbgZQZGarzWyDmd3b2ycys/vNbL2ZrW9oaBhYYgnEysrO/+feOFO/TIlERTKF3tvlD95jOx+YC9wG3AI8bmYzTvsg92XuXuHuFSUlWkQhTCsr67mkdBTjRg4OO4qIBCSZQq8BJnXbngjU9nLMm+7e5O6HgDXAnGAiStAOnWjh0/3HWDhLZ+ciUZJMoa8DysxsmpkVAncBy3sc8ypwrZnlm9lQ4CqgMtioEpTVOxpwR5crikRMv1e5uHu7mT0MvAXEgKfdfZuZPdC1f6m7V5rZm8BmoAN4yt23pjK4DNzKynrGjRzExeePDDuKiAQoqcUj3X0FsKLHa0t7bP8D8A/BRZNUaG3vYM3OBr55WSlmujtUJEp0p2iO+XjPYZpaE9yk+XORyFGh55iVlXEG5edx9QW6O1QkalToOcTdWVUV55oLixlSqLtDRaJGhZ5DPmtoYt+Rk9ygm4lEIkmFnkNWVXWuHaq7Q0WiSYWeQ1ZVxZk5fgSlo4eEHUVEUkCFniMam9tY9/lRnZ2LRJgKPUd8sKuBRIfrdn+RCFOh54hVlXGKhhZw2aSisKOISIqo0HNAosNZvbOBBReNJaa1Q0UiS4WeAzbuP8aRplZdrigScSr0HPBeVZxYnnF9mZ5BLxJlKvQcsLIqztwpRYwaWhB2FBFJIRV6xB1sbKby4Be6XFEkB6jQI+69qs61Wxeq0EUiT4Uecauq6plYNIQLxw4PO4qIpJgKPcJOtSX4j+rD3DhzrBazEMkBKvQIW7v7MM1tCc2fi+QIFXqEraqKM6QgxvzpY8KOIiJpoEKPqN8uZjGGwQVazEIkF6jQI6o6foKao83cOHNc2FFEJE1U6BG1sioOwA0zdXeoSK5QoUfUqqo4syaMZMIoLWYhkitU6BF07GQrG/Ye5SY9+1wkp6jQI+j9nZ2LWehyRZHcokKPoFVVccYMK2TOxNFhRxGRNFKhR0x7ooPVOxq4YeZY8rSYhUhOUaFHzCf7jtHY3KaHcYnkIBV6xKysqqcgZny9rDjsKCKSZir0iFlVGeeqaWMYMViLWYjkGhV6hOw7fJJd8RNaO1QkRyVV6Ga22Mx2mFm1mT12huOuNLOEmd0ZXERJ1qqqekCLWYjkqn4L3cxiwJPAEqAcuNvMyvs47kfAW0GHlOSsrIozvWQYU4uHhR1FREKQzBn6PKDa3Xe7eyvwPHB7L8c9ArwExAPMJ0k6fqqNtbsPs2iWHsYlkquSKfRSYH+37Zqu175kZqXAHcDSM30iM7vfzNab2fqGhoazzSpnsGbnIdoSzkIVukjOSqbQe7s7xXts/xh41N0TZ/pE7r7M3SvcvaKkRE8BDNK7lfUUDS3gismjw44iIiHJT+KYGmBSt+2JQG2PYyqA57vWrSwGbjWzdnd/JYiQcmbtiQ7e2xHnxovGkh/ThUsiuSqZQl8HlJnZNOAAcBdwT/cD3H3ab943s2eA11Xm6bNh71GOnWzjpnJNt4jksn4L3d3bzexhOq9eiQFPu/s2M3uga/8Z580l9VZWxSmM5XHdDE1jieSyZM7QcfcVwIoer/Va5O7+u+ceS87Gu9vruWr6eQwflNS3U0QiShOuWe6zhhPsPtTEIk23iOQ8FXqWW1nZeXeoFrMQERV6lnt3e5yZ40cwsWho2FFEJGQq9Cx26EQL6/ce4ZaLx4cdRUQygAo9i62qjNPhcPPFmj8XERV6Vnt7ex2lo4dQPmFk2FFEJAOo0LNUU0s7a3YdYlH5OLru0BWRHKdCz1If7Gqgtb1D0y0i8iUVepZ6e1s9o4YUMG/qeWFHEZEMoULPQm2JDlZWxVk4Sw/jEpHfUhtkoXV7jtDY3MbN5bpcUUR+S4Wehd7eXs+g/Dyum1EcdhQRySAq9CzT0eG8ta2Oa8tKGFqoh3GJyG+p0LPMpppjHGw8xa2XaLpFRL5KhZ5l3thaR0HMtHaoiJxGhZ5F3J03th7kmguLGTWkIOw4IpJhVOhZZFvtF+w/0sytsyeEHUVEMpAKPYus2HKQWJ5pMQsR6ZUKPUt0TrfU8bXpYygaVhh2HBHJQCr0LLGj/jh7DjWxeLaubhGR3qnQs8QbW+owQ4tZiEifVOhZYsWWg1w59TxKRgwKO4qIZCgVehaoqvuCXfETfONSXd0iIn1ToWeB5RtrieUZSy5RoYtI31ToGc7deW1zLVdfMIbi4ZpuEZG+qdAz3Mb9x9h/pJlvzjk/7CgikuFU6Blu+aZaCmN53KyrW0SkHyr0DJbocH69+SALLirRs1tEpF8q9Az28Z7DxI+38M3LNN0iIv1ToWew1zbVMrQwxsKZenaLiPQvqUI3s8VmtsPMqs3ssV72f8/MNne9fWhmc4KPmlta2hOs2FLHovJxDCmMhR1HRLJAv4VuZjHgSWAJUA7cbWblPQ7bA1zv7pcCTwDLgg6aa1ZVxmlsbuOOy0vDjiIiWSKZM/R5QLW773b3VuB54PbuB7j7h+5+tGtzLTAx2Ji558UNNYwbOYhry0rCjiIiWSKZQi8F9nfbrul6rS+/D7zR2w4zu9/M1pvZ+oaGhuRT5piG4y2s3tnAty4vJZZnYccRkSyRTKH31ije64FmN9BZ6I/2tt/dl7l7hbtXlJTozLMvr248QKLDufMK/aIjIsnLT+KYGmBSt+2JQG3Pg8zsUuApYIm7Hw4mXm56cUMNcyaOomzciLCjiEgWSeYMfR1QZmbTzKwQuAtY3v0AM5sMvAx83913Bh8zd2yrbaSq7jjfnquzcxE5O/2eobt7u5k9DLwFxICn3X2bmT3QtX8p8NfAGOBnZgbQ7u4VqYsdXS9tOEBhLI9vXKqbiUTk7CQz5YK7rwBW9Hhtabf3/wD4g2Cj5Z7W9g5e3XiAhbPGat1QETlrulM0g7y1rY7DTa1858pJ/R8sItKDCj2D/HztXiadN4Trde25iAyACj1DVMeP8/GeI9wzbwp5uvZcRAZAhZ4hfr52HwUx439U6OoWERkYFXoGaG5N8NInNSyZPUHLzInIgKnQM8Brm2o5fqqd35k/JewoIpLFVOgZ4Bcf72XGuOFcObUo7CgiksVU6CHbsPcom2oa+d5VU+i6KUtEZEBU6CFbtuYzRg8t0B9DReScqdBDtLvhBG9vr+f786cwtDCpm3ZFRPqkQg/Rv36wh4JYHvd+bWrYUUQkAlToIWk43sJLn9Tw7SsmUjJClyqKyLlToYfk2Y8+py3RwR9eOy3sKCISESr0EDS1tPPsR3u5uXwc00uGhx1HRCJChR6Cp///Hhqb23jg+gvCjiIiEaJCT7OjTa0sW7ObReXjuHyybiQSkeCo0NNs6fufcaK1nf9980VhRxGRiFGhp1Fd4yme+fBz7rislIvGawFoEQmWCj2NfrJqF4kO53/eNCPsKCISQSr0NPms4QQvrNvP3fMmM3nM0LDjiEgEqdDTwN35q19tZUhhjEcWXhh2HBGJKBV6Gry4oYaPdh/mL5bMYuyIwWHHEZGIUqGn2OETLfztikoqphRx15WTwo4jIhGmQk+xv/11JU0t7fzdf79Eiz+LSEqp0FPo3e31vPzpAR64/gLKxukyRRFJLRV6iuw51MT/emEjs0tH8tAN+kOoiKSeCj0Fmlra+aN/X09+nrH0d+YyuCAWdiQRyQFaJidg7s6fv7SZ6vgJnv29q5hYpGvORSQ9dIYeoI4O54nXK/n15oP8+eKZfL2sOOxIIpJDdIYekLZEB4++uJmXPz3A7149lT+6bnrYkUQkx6jQA9DcmuCR5z7h3co4f7poBo/ceCFmukRRRNIrqSkXM1tsZjvMrNrMHutlv5nZT7r2bzazK4KPmnncneWbaln4j6tZWRXniW/N5gcLy1TmIhKKfs/QzSwGPAksAmqAdWa23N23dztsCVDW9XYV8M9d/40cd2fPoSY27j/GLz7ex4a9RymfMJJ/+u5lzJ8+Jux4IpLDkplymQdUu/tuADN7Hrgd6F7otwPPursDa81stJlNcPeDQQd+f2cDf/P69v4PDJDTWeQAh0600tjcBkDJiEH86NuXcOfcScR0F6iIhCyZQi8F9nfbruH0s+/ejikFvlLoZnY/cD/A5MmTzzYrAMMH5VM2Lv0LKxsGBiMH5zNn4mgun1zEhWOHq8hFJGMkU+i9NZYP4BjcfRmwDKCiouK0/cmYO6WIuVPmDuRDRUQiLZk/itYA3R8TOBGoHcAxIiKSQskU+jqgzMymmVkhcBewvMcxy4F7u652mQ80pmL+XERE+tbvlIu7t5vZw8BbQAx42t23mdkDXfuXAiuAW4Fq4CRwX+oii4hIb5K6scjdV9BZ2t1fW9rtfQceCjaaiIicDT3LRUQkIlToIiIRoUIXEYkIFbqISETYb25pT/sXNmsA9g7ww4uBQwHGyQYac27QmHPDuYx5iruX9LYjtEI/F2a23t0rws6RThpzbtCYc0OqxqwpFxGRiFChi4hERLYW+rKwA4RAY84NGnNuSMmYs3IOXURETpetZ+giItKDCl1EJCIyutBzcXHqJMb8va6xbjazD81sThg5g9TfmLsdd6WZJczsznTmS4VkxmxmC8xso5ltM7P3050xaEn82x5lZq+Z2aauMWf1U1vN7Gkzi5vZ1j72B99f7p6Rb3Q+qvczYDpQCGwCynsccyvwBp0rJs0HPg47dxrGfDVQ1PX+klwYc7fjVtH51M87w86dhu/zaDrX7Z3ctT027NxpGPNfAj/qer8EOAIUhp39HMZ8HXAFsLWP/YH3VyafoX+5OLW7twK/WZy6uy8Xp3b3tcBoM5uQ7qAB6nfM7v6hux/t2lxL5+pQ2SyZ7zPAI8BLQDyd4VIkmTHfA7zs7vsA3D3bx53MmB0YYWYGDKez0NvTGzM47r6GzjH0JfD+yuRC72vh6bM9Jpuc7Xh+n87/w2ezfsdsZqXAHcBSoiGZ7/MMoMjMVpvZBjO7N23pUiOZMf8UmEXn8pVbgD9x9470xAtF4P2V1AIXIQlsceoskvR4zOwGOgv96ylNlHrJjPnHwKPunug8ect6yYw5H5gLLASGAB+Z2Vp335nqcCmSzJhvATYCNwIXAO+Y2Qfu/kWKs4Ul8P7K5ELPxcWpkxqPmV0KPAUscffDacqWKsmMuQJ4vqvMi4Fbzazd3V9JS8LgJftv+5C7NwFNZrYGmANka6EnM+b7gL/3zgnmajPbA8wE/jM9EdMu8P7K5CmXXFycut8xm9lk4GXg+1l8ttZdv2N292nuPtXdpwIvAg9mcZlDcv+2XwWuNbN8MxsKXAVUpjlnkJIZ8z46fyPBzMYBFwG705oyvQLvr4w9Q/ccXJw6yTH/NTAG+FnXGWu7Z/GT6pIcc6QkM2Z3rzSzN4HNQAfwlLv3evlbNkjy+/wE8IyZbaFzOuJRd8/ax+qa2XPAAqDYzGqAHwIFkLr+0q3/IiIRkclTLiIichZU6CIiEaFCFxGJCBW6iEhEqNBFRCJChS4iEhEqdBGRiPgvN02yqBr+QIAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AnnealedLossCallback(Callback):\n",
    "    def after_pred(self):\n",
    "        kl = self.learn.pred[0].new(1)\n",
    "        kl[0] = self.opt.hypers[0]['kl_weight']\n",
    "        self.learn.pred = self.learn.pred + (kl,)\n",
    "    def after_batch(self):\n",
    "        if(len(self.learn.pred)):\n",
    "            cats, conts, mu, logvar, _ = self.learn.pred\n",
    "        else:\n",
    "            cats, conts, mu, logvar = self.learn.pred\n",
    "            \n",
    "        self.learn.pred = (cats, conts, mu, logvar)\n",
    "        \n",
    "f = combine_scheds([.1, .3, .6], [SchedCos(0,0), SchedCos(0,1), SchedNo(1,1)])\n",
    "p = torch.linspace(0.,1,100)\n",
    "plt.plot(p,[f(o) for o in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrhU2fLP7N-U"
   },
   "source": [
    "We'll make an config dictionary for us to use as a list of all hyper parameters.  Also I would recommend against using early stopping because our AnnealedLossCallback will make the loss go worst once the KL divergence weight become larger than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "ThV0ch4x7R58"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'hidden_size': 128,\n",
    "    'dropout': 0.0,\n",
    "    'embed_p': 0.0,\n",
    "    'wd': 0.01,\n",
    "    'bswap': 0.1,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "cbs = [ParamScheduler({'kl_weight': f }), AnnealedLossCallback()]\n",
    "metrics = [MSEMetric(), CEMetric(), KLDMetric(), MUMetric(), StdMetric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYgESCPk7TeP"
   },
   "source": [
    "And make our model & `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "i7xmhROaEdOw"
   },
   "outputs": [],
   "source": [
    "model = TabularVAE(emb_szs, len(cont_names), config['hidden_size'], ps=config['dropout'], cats=total_cats, embed_p=config['embed_p'], bswap=config['bswap'], low=tensor(low).cuda(), high=tensor(high).cuda())\n",
    "learn = Learner(dls, model, lr=config['lr'], loss_func=loss_func, wd=config['wd'], opt_func=ranger, cbs=cbs, metrics=metrics).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J76BTkMP7Ws2"
   },
   "source": [
    "Finally we'll fit for a few epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "dcFlPHLAGBZ4",
    "outputId": "07d78f9a-b513-4b89-fc0c-cbfcc02db820"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>ce</th>\n",
       "      <th>kld</th>\n",
       "      <th>mu</th>\n",
       "      <th>std</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>315.605652</td>\n",
       "      <td>129.085739</td>\n",
       "      <td>1875.5427</td>\n",
       "      <td>1901.0209</td>\n",
       "      <td>18535.082</td>\n",
       "      <td>0.0017046062</td>\n",
       "      <td>1.4117877</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>176.394974</td>\n",
       "      <td>75.675156</td>\n",
       "      <td>1545.153</td>\n",
       "      <td>1450.1143</td>\n",
       "      <td>62741.055</td>\n",
       "      <td>-0.012652379</td>\n",
       "      <td>1.3945049</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>123.456360</td>\n",
       "      <td>64.680702</td>\n",
       "      <td>1439.2533</td>\n",
       "      <td>1188.363</td>\n",
       "      <td>123300.29</td>\n",
       "      <td>-0.021421198</td>\n",
       "      <td>1.3422272</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>97.023422</td>\n",
       "      <td>57.760181</td>\n",
       "      <td>1387.2825</td>\n",
       "      <td>1005.8914</td>\n",
       "      <td>213299.9</td>\n",
       "      <td>-0.028006313</td>\n",
       "      <td>1.2807553</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>81.636597</td>\n",
       "      <td>51.438267</td>\n",
       "      <td>1348.5676</td>\n",
       "      <td>866.9492</td>\n",
       "      <td>281948.94</td>\n",
       "      <td>-0.032968253</td>\n",
       "      <td>1.234925</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>71.761780</td>\n",
       "      <td>47.578621</td>\n",
       "      <td>1322.5017</td>\n",
       "      <td>755.7578</td>\n",
       "      <td>342068.38</td>\n",
       "      <td>-0.036913283</td>\n",
       "      <td>1.2001842</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>65.135506</td>\n",
       "      <td>46.210411</td>\n",
       "      <td>1305.7738</td>\n",
       "      <td>668.72626</td>\n",
       "      <td>411158.25</td>\n",
       "      <td>-0.040174223</td>\n",
       "      <td>1.1736237</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>60.570484</td>\n",
       "      <td>44.721779</td>\n",
       "      <td>1291.7133</td>\n",
       "      <td>599.52313</td>\n",
       "      <td>486491.78</td>\n",
       "      <td>-0.04356434</td>\n",
       "      <td>1.1530076</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>57.616035</td>\n",
       "      <td>43.848965</td>\n",
       "      <td>1280.4219</td>\n",
       "      <td>543.21704</td>\n",
       "      <td>553554.44</td>\n",
       "      <td>-0.045224637</td>\n",
       "      <td>1.1367451</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>55.411850</td>\n",
       "      <td>43.521729</td>\n",
       "      <td>1271.6382</td>\n",
       "      <td>496.96478</td>\n",
       "      <td>633306.1</td>\n",
       "      <td>-0.047541678</td>\n",
       "      <td>1.1235219</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>54.464172</td>\n",
       "      <td>45.415272</td>\n",
       "      <td>1264.1826</td>\n",
       "      <td>457.89178</td>\n",
       "      <td>658026.94</td>\n",
       "      <td>-0.047140382</td>\n",
       "      <td>1.1128831</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>54.388256</td>\n",
       "      <td>48.253239</td>\n",
       "      <td>1259.8339</td>\n",
       "      <td>430.87473</td>\n",
       "      <td>619856.3</td>\n",
       "      <td>-0.043366022</td>\n",
       "      <td>1.1055535</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>54.615913</td>\n",
       "      <td>46.875431</td>\n",
       "      <td>1255.5734</td>\n",
       "      <td>404.71124</td>\n",
       "      <td>579995.06</td>\n",
       "      <td>-0.040178657</td>\n",
       "      <td>1.0988932</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>54.998371</td>\n",
       "      <td>48.024719</td>\n",
       "      <td>1251.8092</td>\n",
       "      <td>383.03607</td>\n",
       "      <td>543972.75</td>\n",
       "      <td>-0.037912704</td>\n",
       "      <td>1.0924997</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>55.746094</td>\n",
       "      <td>49.857548</td>\n",
       "      <td>1249.3494</td>\n",
       "      <td>365.30423</td>\n",
       "      <td>511781.56</td>\n",
       "      <td>-0.035420142</td>\n",
       "      <td>1.0866311</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>56.661739</td>\n",
       "      <td>50.328270</td>\n",
       "      <td>1247.1434</td>\n",
       "      <td>350.32346</td>\n",
       "      <td>482594.78</td>\n",
       "      <td>-0.03294841</td>\n",
       "      <td>1.0814266</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>57.603569</td>\n",
       "      <td>50.386925</td>\n",
       "      <td>1244.8528</td>\n",
       "      <td>336.25565</td>\n",
       "      <td>456484.8</td>\n",
       "      <td>-0.031101061</td>\n",
       "      <td>1.0767891</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>58.583523</td>\n",
       "      <td>51.981812</td>\n",
       "      <td>1243.4575</td>\n",
       "      <td>324.57477</td>\n",
       "      <td>433010.84</td>\n",
       "      <td>-0.029394638</td>\n",
       "      <td>1.0726302</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>59.556587</td>\n",
       "      <td>53.178230</td>\n",
       "      <td>1241.8759</td>\n",
       "      <td>315.17038</td>\n",
       "      <td>411829.2</td>\n",
       "      <td>-0.028038898</td>\n",
       "      <td>1.0688814</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>60.545700</td>\n",
       "      <td>54.620186</td>\n",
       "      <td>1241.0029</td>\n",
       "      <td>307.1985</td>\n",
       "      <td>392633.25</td>\n",
       "      <td>-0.02676288</td>\n",
       "      <td>1.0654918</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>61.632618</td>\n",
       "      <td>55.850784</td>\n",
       "      <td>1240.5299</td>\n",
       "      <td>301.0517</td>\n",
       "      <td>375091.53</td>\n",
       "      <td>-0.025586355</td>\n",
       "      <td>1.0624204</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>62.557167</td>\n",
       "      <td>56.964153</td>\n",
       "      <td>1239.929</td>\n",
       "      <td>295.78165</td>\n",
       "      <td>359111.22</td>\n",
       "      <td>-0.024614256</td>\n",
       "      <td>1.0596149</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>63.625866</td>\n",
       "      <td>56.889721</td>\n",
       "      <td>1239.3777</td>\n",
       "      <td>290.45956</td>\n",
       "      <td>344423.1</td>\n",
       "      <td>-0.023640629</td>\n",
       "      <td>1.0570537</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>64.514572</td>\n",
       "      <td>58.110432</td>\n",
       "      <td>1239.0974</td>\n",
       "      <td>286.23486</td>\n",
       "      <td>330899.94</td>\n",
       "      <td>-0.022748372</td>\n",
       "      <td>1.0547007</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>65.390701</td>\n",
       "      <td>59.560081</td>\n",
       "      <td>1238.9471</td>\n",
       "      <td>283.353</td>\n",
       "      <td>318412.5</td>\n",
       "      <td>-0.021903502</td>\n",
       "      <td>1.0525341</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>66.237366</td>\n",
       "      <td>60.684116</td>\n",
       "      <td>1239.093</td>\n",
       "      <td>281.1641</td>\n",
       "      <td>306846.38</td>\n",
       "      <td>-0.021238824</td>\n",
       "      <td>1.050533</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>67.022224</td>\n",
       "      <td>61.788044</td>\n",
       "      <td>1239.3357</td>\n",
       "      <td>279.65836</td>\n",
       "      <td>296110.66</td>\n",
       "      <td>-0.020508835</td>\n",
       "      <td>1.0486774</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>67.920181</td>\n",
       "      <td>62.705631</td>\n",
       "      <td>1239.6572</td>\n",
       "      <td>278.7628</td>\n",
       "      <td>286112.56</td>\n",
       "      <td>-0.019838035</td>\n",
       "      <td>1.0469552</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>68.607430</td>\n",
       "      <td>62.080826</td>\n",
       "      <td>1239.6876</td>\n",
       "      <td>277.51285</td>\n",
       "      <td>276767.3</td>\n",
       "      <td>-0.019170845</td>\n",
       "      <td>1.0453478</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>69.229019</td>\n",
       "      <td>63.644730</td>\n",
       "      <td>1239.9357</td>\n",
       "      <td>277.1073</td>\n",
       "      <td>268036.97</td>\n",
       "      <td>-0.018629855</td>\n",
       "      <td>1.0438461</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>70.084190</td>\n",
       "      <td>64.530296</td>\n",
       "      <td>1240.5626</td>\n",
       "      <td>276.69025</td>\n",
       "      <td>259862.53</td>\n",
       "      <td>-0.018059881</td>\n",
       "      <td>1.0424391</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>70.840187</td>\n",
       "      <td>66.210960</td>\n",
       "      <td>1240.9869</td>\n",
       "      <td>277.59714</td>\n",
       "      <td>252191.83</td>\n",
       "      <td>-0.017514495</td>\n",
       "      <td>1.041121</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>71.791565</td>\n",
       "      <td>66.886604</td>\n",
       "      <td>1241.7402</td>\n",
       "      <td>278.5646</td>\n",
       "      <td>244972.1</td>\n",
       "      <td>-0.017007822</td>\n",
       "      <td>1.0398829</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>72.509125</td>\n",
       "      <td>67.280312</td>\n",
       "      <td>1242.4683</td>\n",
       "      <td>279.45413</td>\n",
       "      <td>238173.98</td>\n",
       "      <td>-0.016590409</td>\n",
       "      <td>1.038718</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>72.985466</td>\n",
       "      <td>66.199097</td>\n",
       "      <td>1242.9928</td>\n",
       "      <td>279.90115</td>\n",
       "      <td>231740.17</td>\n",
       "      <td>-0.0161453</td>\n",
       "      <td>1.0376192</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>73.174545</td>\n",
       "      <td>66.051682</td>\n",
       "      <td>1243.4445</td>\n",
       "      <td>280.22534</td>\n",
       "      <td>225655.52</td>\n",
       "      <td>-0.015740085</td>\n",
       "      <td>1.0365807</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>73.475906</td>\n",
       "      <td>67.038330</td>\n",
       "      <td>1244.0173</td>\n",
       "      <td>280.88092</td>\n",
       "      <td>219902.34</td>\n",
       "      <td>-0.015329599</td>\n",
       "      <td>1.035596</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>73.572441</td>\n",
       "      <td>66.639206</td>\n",
       "      <td>1244.3884</td>\n",
       "      <td>281.54727</td>\n",
       "      <td>214441.52</td>\n",
       "      <td>-0.014926243</td>\n",
       "      <td>1.0346632</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>73.694786</td>\n",
       "      <td>68.201500</td>\n",
       "      <td>1245.1456</td>\n",
       "      <td>282.75552</td>\n",
       "      <td>209264.17</td>\n",
       "      <td>-0.014588745</td>\n",
       "      <td>1.0337788</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>73.887871</td>\n",
       "      <td>67.923775</td>\n",
       "      <td>1245.6869</td>\n",
       "      <td>283.58884</td>\n",
       "      <td>204353.97</td>\n",
       "      <td>-0.014205694</td>\n",
       "      <td>1.0329369</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>73.850479</td>\n",
       "      <td>66.239357</td>\n",
       "      <td>1245.9128</td>\n",
       "      <td>283.82257</td>\n",
       "      <td>199672.03</td>\n",
       "      <td>-0.013906971</td>\n",
       "      <td>1.0321369</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>73.782158</td>\n",
       "      <td>66.429039</td>\n",
       "      <td>1246.0859</td>\n",
       "      <td>284.2725</td>\n",
       "      <td>195211.34</td>\n",
       "      <td>-0.013625735</td>\n",
       "      <td>1.0313747</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>73.653885</td>\n",
       "      <td>66.659859</td>\n",
       "      <td>1246.4539</td>\n",
       "      <td>284.7323</td>\n",
       "      <td>190955.7</td>\n",
       "      <td>-0.013295535</td>\n",
       "      <td>1.0306474</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>73.566811</td>\n",
       "      <td>66.856720</td>\n",
       "      <td>1246.69</td>\n",
       "      <td>285.19553</td>\n",
       "      <td>186900.55</td>\n",
       "      <td>-0.013030132</td>\n",
       "      <td>1.029953</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>73.417664</td>\n",
       "      <td>66.686462</td>\n",
       "      <td>1246.8904</td>\n",
       "      <td>285.51553</td>\n",
       "      <td>183026.81</td>\n",
       "      <td>-0.01275124</td>\n",
       "      <td>1.0292895</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>73.347862</td>\n",
       "      <td>66.899437</td>\n",
       "      <td>1247.1626</td>\n",
       "      <td>286.03958</td>\n",
       "      <td>179316.31</td>\n",
       "      <td>-0.012550251</td>\n",
       "      <td>1.0286547</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>73.254814</td>\n",
       "      <td>65.639450</td>\n",
       "      <td>1247.2694</td>\n",
       "      <td>286.05338</td>\n",
       "      <td>175759.17</td>\n",
       "      <td>-0.012282635</td>\n",
       "      <td>1.0280468</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>73.498413</td>\n",
       "      <td>67.907257</td>\n",
       "      <td>1247.5518</td>\n",
       "      <td>286.66025</td>\n",
       "      <td>172369.6</td>\n",
       "      <td>-0.012043736</td>\n",
       "      <td>1.0274642</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>74.008057</td>\n",
       "      <td>67.339485</td>\n",
       "      <td>1248.0586</td>\n",
       "      <td>287.00015</td>\n",
       "      <td>169107.8</td>\n",
       "      <td>-0.011833687</td>\n",
       "      <td>1.0269059</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>73.952530</td>\n",
       "      <td>66.876701</td>\n",
       "      <td>1248.3171</td>\n",
       "      <td>287.31464</td>\n",
       "      <td>165975.47</td>\n",
       "      <td>-0.011567705</td>\n",
       "      <td>1.0263697</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>73.670166</td>\n",
       "      <td>66.616814</td>\n",
       "      <td>1248.7125</td>\n",
       "      <td>287.62408</td>\n",
       "      <td>162956.34</td>\n",
       "      <td>-0.011330355</td>\n",
       "      <td>1.0258546</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>73.379807</td>\n",
       "      <td>66.764999</td>\n",
       "      <td>1248.791</td>\n",
       "      <td>287.94904</td>\n",
       "      <td>160064.69</td>\n",
       "      <td>-0.011125071</td>\n",
       "      <td>1.0253589</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>73.307800</td>\n",
       "      <td>65.720436</td>\n",
       "      <td>1248.8961</td>\n",
       "      <td>287.92343</td>\n",
       "      <td>157273.66</td>\n",
       "      <td>-0.010915935</td>\n",
       "      <td>1.0248817</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>73.131393</td>\n",
       "      <td>65.845245</td>\n",
       "      <td>1248.9801</td>\n",
       "      <td>287.9351</td>\n",
       "      <td>154587.53</td>\n",
       "      <td>-0.0107346745</td>\n",
       "      <td>1.0244218</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>72.925865</td>\n",
       "      <td>66.152550</td>\n",
       "      <td>1249.0571</td>\n",
       "      <td>288.00815</td>\n",
       "      <td>152002.47</td>\n",
       "      <td>-0.010556355</td>\n",
       "      <td>1.0239786</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>72.957619</td>\n",
       "      <td>66.478058</td>\n",
       "      <td>1249.1141</td>\n",
       "      <td>288.0837</td>\n",
       "      <td>149515.48</td>\n",
       "      <td>-0.01038503</td>\n",
       "      <td>1.0235512</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>72.835587</td>\n",
       "      <td>66.314011</td>\n",
       "      <td>1249.2328</td>\n",
       "      <td>288.05783</td>\n",
       "      <td>147114.22</td>\n",
       "      <td>-0.010186567</td>\n",
       "      <td>1.0231389</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>72.841858</td>\n",
       "      <td>66.599098</td>\n",
       "      <td>1249.5593</td>\n",
       "      <td>288.08066</td>\n",
       "      <td>144792.06</td>\n",
       "      <td>-0.010022131</td>\n",
       "      <td>1.0227408</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>72.846962</td>\n",
       "      <td>65.597755</td>\n",
       "      <td>1249.5917</td>\n",
       "      <td>288.04675</td>\n",
       "      <td>142543.62</td>\n",
       "      <td>-0.009859407</td>\n",
       "      <td>1.0223563</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>72.701073</td>\n",
       "      <td>67.177979</td>\n",
       "      <td>1250.2264</td>\n",
       "      <td>288.18268</td>\n",
       "      <td>140370.06</td>\n",
       "      <td>-0.009697698</td>\n",
       "      <td>1.0219846</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>73.099808</td>\n",
       "      <td>66.945946</td>\n",
       "      <td>1250.6053</td>\n",
       "      <td>288.517</td>\n",
       "      <td>138265.25</td>\n",
       "      <td>-0.00953056</td>\n",
       "      <td>1.0216253</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>73.258827</td>\n",
       "      <td>66.379417</td>\n",
       "      <td>1250.7235</td>\n",
       "      <td>288.68152</td>\n",
       "      <td>136232.81</td>\n",
       "      <td>-0.009402944</td>\n",
       "      <td>1.0212777</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>73.100410</td>\n",
       "      <td>66.371780</td>\n",
       "      <td>1250.8433</td>\n",
       "      <td>288.85992</td>\n",
       "      <td>134264.02</td>\n",
       "      <td>-0.009260478</td>\n",
       "      <td>1.0209413</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>73.062691</td>\n",
       "      <td>66.874161</td>\n",
       "      <td>1251.087</td>\n",
       "      <td>289.14206</td>\n",
       "      <td>132356.48</td>\n",
       "      <td>-0.00913958</td>\n",
       "      <td>1.0206153</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>72.902321</td>\n",
       "      <td>65.461472</td>\n",
       "      <td>1251.0798</td>\n",
       "      <td>289.03195</td>\n",
       "      <td>130507.38</td>\n",
       "      <td>-0.009002098</td>\n",
       "      <td>1.020299</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>72.830818</td>\n",
       "      <td>65.613754</td>\n",
       "      <td>1251.0726</td>\n",
       "      <td>289.00052</td>\n",
       "      <td>128714.06</td>\n",
       "      <td>-0.008873075</td>\n",
       "      <td>1.0199922</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>72.778915</td>\n",
       "      <td>65.820244</td>\n",
       "      <td>1251.1099</td>\n",
       "      <td>288.959</td>\n",
       "      <td>126976.086</td>\n",
       "      <td>-0.008747158</td>\n",
       "      <td>1.0196944</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>72.672302</td>\n",
       "      <td>66.572350</td>\n",
       "      <td>1251.158</td>\n",
       "      <td>289.24652</td>\n",
       "      <td>125288.71</td>\n",
       "      <td>-0.00863287</td>\n",
       "      <td>1.0194057</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>72.705124</td>\n",
       "      <td>66.424759</td>\n",
       "      <td>1251.2244</td>\n",
       "      <td>289.3599</td>\n",
       "      <td>123652.91</td>\n",
       "      <td>-0.008518713</td>\n",
       "      <td>1.0191252</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>72.848953</td>\n",
       "      <td>66.487564</td>\n",
       "      <td>1251.1248</td>\n",
       "      <td>289.47577</td>\n",
       "      <td>122069.7</td>\n",
       "      <td>-0.008377662</td>\n",
       "      <td>1.0188526</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>72.752525</td>\n",
       "      <td>64.692032</td>\n",
       "      <td>1250.4094</td>\n",
       "      <td>289.74554</td>\n",
       "      <td>120522.24</td>\n",
       "      <td>-0.00825889</td>\n",
       "      <td>1.0185878</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>70.956314</td>\n",
       "      <td>54.567883</td>\n",
       "      <td>1244.738</td>\n",
       "      <td>290.71353</td>\n",
       "      <td>119022.74</td>\n",
       "      <td>-0.008151018</td>\n",
       "      <td>1.0183325</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>63.571449</td>\n",
       "      <td>40.391094</td>\n",
       "      <td>1233.719</td>\n",
       "      <td>291.38382</td>\n",
       "      <td>117567.28</td>\n",
       "      <td>-0.008038265</td>\n",
       "      <td>1.0180829</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>55.677109</td>\n",
       "      <td>34.603676</td>\n",
       "      <td>1220.3777</td>\n",
       "      <td>292.1673</td>\n",
       "      <td>116157.52</td>\n",
       "      <td>-0.007923341</td>\n",
       "      <td>1.0178391</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>49.834736</td>\n",
       "      <td>33.361252</td>\n",
       "      <td>1207.0188</td>\n",
       "      <td>292.84875</td>\n",
       "      <td>114784.41</td>\n",
       "      <td>-0.0077933664</td>\n",
       "      <td>1.0176016</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>45.865498</td>\n",
       "      <td>32.409042</td>\n",
       "      <td>1193.893</td>\n",
       "      <td>293.19043</td>\n",
       "      <td>113449.76</td>\n",
       "      <td>-0.007697153</td>\n",
       "      <td>1.0173702</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>43.182312</td>\n",
       "      <td>31.046616</td>\n",
       "      <td>1180.8247</td>\n",
       "      <td>293.38693</td>\n",
       "      <td>112146.67</td>\n",
       "      <td>-0.007611273</td>\n",
       "      <td>1.0171452</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>41.398602</td>\n",
       "      <td>31.244078</td>\n",
       "      <td>1168.1373</td>\n",
       "      <td>293.5076</td>\n",
       "      <td>110880.15</td>\n",
       "      <td>-0.0075096106</td>\n",
       "      <td>1.0169256</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>40.294785</td>\n",
       "      <td>31.232410</td>\n",
       "      <td>1155.8362</td>\n",
       "      <td>293.63776</td>\n",
       "      <td>109643.08</td>\n",
       "      <td>-0.0074239355</td>\n",
       "      <td>1.0167117</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>39.632290</td>\n",
       "      <td>31.169394</td>\n",
       "      <td>1143.7474</td>\n",
       "      <td>293.7762</td>\n",
       "      <td>108438.87</td>\n",
       "      <td>-0.0073237317</td>\n",
       "      <td>1.016503</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>39.160358</td>\n",
       "      <td>30.819849</td>\n",
       "      <td>1131.8271</td>\n",
       "      <td>293.75378</td>\n",
       "      <td>107269.51</td>\n",
       "      <td>-0.007232972</td>\n",
       "      <td>1.0162996</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>38.656021</td>\n",
       "      <td>30.902313</td>\n",
       "      <td>1120.3805</td>\n",
       "      <td>293.76337</td>\n",
       "      <td>106122.8</td>\n",
       "      <td>-0.007195173</td>\n",
       "      <td>1.0161011</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>38.401901</td>\n",
       "      <td>30.204983</td>\n",
       "      <td>1108.9576</td>\n",
       "      <td>293.7057</td>\n",
       "      <td>105006.04</td>\n",
       "      <td>-0.007099481</td>\n",
       "      <td>1.0159073</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>38.164410</td>\n",
       "      <td>29.914957</td>\n",
       "      <td>1097.8491</td>\n",
       "      <td>293.53955</td>\n",
       "      <td>103914.805</td>\n",
       "      <td>-0.0070017967</td>\n",
       "      <td>1.0157181</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>37.958992</td>\n",
       "      <td>30.432314</td>\n",
       "      <td>1087.0907</td>\n",
       "      <td>293.49197</td>\n",
       "      <td>102848.445</td>\n",
       "      <td>-0.0069153705</td>\n",
       "      <td>1.0155334</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>37.750439</td>\n",
       "      <td>29.916395</td>\n",
       "      <td>1076.5225</td>\n",
       "      <td>293.3804</td>\n",
       "      <td>101805.28</td>\n",
       "      <td>-0.006817543</td>\n",
       "      <td>1.0153532</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>37.575119</td>\n",
       "      <td>30.200594</td>\n",
       "      <td>1066.2052</td>\n",
       "      <td>293.34576</td>\n",
       "      <td>100786.516</td>\n",
       "      <td>-0.0067211143</td>\n",
       "      <td>1.0151769</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>37.440540</td>\n",
       "      <td>29.987646</td>\n",
       "      <td>1056.0994</td>\n",
       "      <td>293.3184</td>\n",
       "      <td>99789.18</td>\n",
       "      <td>-0.0066496963</td>\n",
       "      <td>1.0150048</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>37.286278</td>\n",
       "      <td>29.704439</td>\n",
       "      <td>1046.1041</td>\n",
       "      <td>293.24496</td>\n",
       "      <td>98816.484</td>\n",
       "      <td>-0.006573904</td>\n",
       "      <td>1.0148363</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>37.233124</td>\n",
       "      <td>29.783121</td>\n",
       "      <td>1036.3579</td>\n",
       "      <td>293.13223</td>\n",
       "      <td>97866.664</td>\n",
       "      <td>-0.0064945593</td>\n",
       "      <td>1.0146717</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>37.167458</td>\n",
       "      <td>29.611338</td>\n",
       "      <td>1026.8456</td>\n",
       "      <td>293.00018</td>\n",
       "      <td>96936.04</td>\n",
       "      <td>-0.0064151124</td>\n",
       "      <td>1.0145106</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>37.067696</td>\n",
       "      <td>29.650030</td>\n",
       "      <td>1017.5097</td>\n",
       "      <td>292.87265</td>\n",
       "      <td>96026.945</td>\n",
       "      <td>-0.006335785</td>\n",
       "      <td>1.0143532</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>36.873672</td>\n",
       "      <td>29.308992</td>\n",
       "      <td>1008.33514</td>\n",
       "      <td>292.70184</td>\n",
       "      <td>95136.7</td>\n",
       "      <td>-0.0062639927</td>\n",
       "      <td>1.014199</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>36.801075</td>\n",
       "      <td>29.442066</td>\n",
       "      <td>999.41016</td>\n",
       "      <td>292.49905</td>\n",
       "      <td>94266.125</td>\n",
       "      <td>-0.006194989</td>\n",
       "      <td>1.0140481</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>36.718876</td>\n",
       "      <td>29.241985</td>\n",
       "      <td>990.6312</td>\n",
       "      <td>292.29044</td>\n",
       "      <td>93413.56</td>\n",
       "      <td>-0.0061219186</td>\n",
       "      <td>1.0139004</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>36.619389</td>\n",
       "      <td>29.347790</td>\n",
       "      <td>982.0413</td>\n",
       "      <td>292.13168</td>\n",
       "      <td>92578.15</td>\n",
       "      <td>-0.006049969</td>\n",
       "      <td>1.0137558</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>36.514511</td>\n",
       "      <td>29.059055</td>\n",
       "      <td>973.56354</td>\n",
       "      <td>291.9166</td>\n",
       "      <td>91761.16</td>\n",
       "      <td>-0.0059831096</td>\n",
       "      <td>1.013614</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>36.442692</td>\n",
       "      <td>29.098783</td>\n",
       "      <td>965.2883</td>\n",
       "      <td>291.7135</td>\n",
       "      <td>90960.02</td>\n",
       "      <td>-0.005917855</td>\n",
       "      <td>1.0134753</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>36.408916</td>\n",
       "      <td>29.255053</td>\n",
       "      <td>957.2215</td>\n",
       "      <td>291.51935</td>\n",
       "      <td>90175.086</td>\n",
       "      <td>-0.0058519444</td>\n",
       "      <td>1.0133394</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>36.427567</td>\n",
       "      <td>29.188887</td>\n",
       "      <td>949.27783</td>\n",
       "      <td>291.34064</td>\n",
       "      <td>89406.08</td>\n",
       "      <td>-0.005788693</td>\n",
       "      <td>1.0132061</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_flat_cos(config['epochs'], lr=4e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6NIV2T7EdO2"
   },
   "source": [
    "# Getting the compressed representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E9LxZUPPJWX"
   },
   "source": [
    "Next we're going to grade our compressed representations and then attempt to train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "bnnQC-jbEdO2"
   },
   "outputs": [],
   "source": [
    "dl = learn.dls.test_dl(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5vf0sGyPTlS"
   },
   "source": [
    "Let's predict over all the data manually using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "LG7l2jCVPV5k"
   },
   "outputs": [],
   "source": [
    "outs = []\n",
    "for batch in dl:\n",
    "    with torch.no_grad():\n",
    "        learn.model.eval()\n",
    "        learn.model.cuda()\n",
    "        out = learn.model(*batch[:2], True).cpu().numpy()\n",
    "        outs.append(out)\n",
    "outs = np.concatenate(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yk4-bKfAPYxR",
    "outputId": "8b4db2ca-feee-44ee-88be-be8d3914c458"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 128)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIjJ0MOjP8ge"
   },
   "source": [
    "As well as get the actual preds and targs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_sLsBpFfQADo",
    "outputId": "5a65abdb-6edc-4a94-f4b8-71ae3a9c3de7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(cat_preds, cont_preds, mu, logvar), (cat_targs, cont_targs) = learn.get_preds(dl=dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykgzRNmcEdO-"
   },
   "source": [
    "# Measuring accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsEY3yF_EdO-"
   },
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "fejwMQRbEdO_",
    "outputId": "ce6d5ba6-17a6-4028-9fff-4bbb2f1911a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroupBy</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Min</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.076</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Max</td>\n",
       "      <td>62.191</td>\n",
       "      <td>360954.983</td>\n",
       "      <td>10.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>4.637</td>\n",
       "      <td>41901.621</td>\n",
       "      <td>0.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Median</td>\n",
       "      <td>3.698</td>\n",
       "      <td>34951.056</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GroupBy    age     fnlwgt  education-num\n",
       "0     Min  0.000      1.076          0.000\n",
       "0     Max 62.191 360954.983         10.133\n",
       "0    Mean  4.637  41901.621          0.374\n",
       "0  Median  3.698  34951.056          0.181\n",
       "0      R2  0.800      0.745          0.933"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "cont_preds = pd.DataFrame(cont_preds, columns=cont_names)\n",
    "cont_targs = pd.DataFrame(cont_targs, columns=cont_names)\n",
    "\n",
    "preds = pd.DataFrame((cont_preds.values * stds.values) + means.values, columns=cont_preds.columns)\n",
    "targets = pd.DataFrame((cont_targs.values * stds.values) + means.values, columns=cont_targs.columns)\n",
    "\n",
    "mi = (np.abs(targets-preds)).min().to_frame().T\n",
    "ma = (np.abs(targets-preds)).max().to_frame().T\n",
    "mean = (np.abs(targets-preds)).mean().to_frame().T\n",
    "median = (np.abs(targets-preds)).median().to_frame().T\n",
    "r2 = pd.DataFrame.from_dict({c:[r2_score(targets[c], preds[c])] for c in preds.columns})\n",
    "\n",
    "\n",
    "for d,name in zip([mi,ma,mean,median,r2], ['Min', 'Max', 'Mean', 'Median', 'R2']):\n",
    "    d = d.insert(0, 'GroupBy', name)\n",
    "    \n",
    "data = pd.concat([mi,ma,mean,median,r2])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xgg2KiaTQIre"
   },
   "source": [
    "We can also grab the R2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "nUVqCx3YEdPB",
    "outputId": "25dcd1b9-7142-4b09-988f-16bba3ea2e6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.826\n",
       "dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXB3fiBrEdPC"
   },
   "source": [
    "## Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "nOEzxfpREdPF"
   },
   "outputs": [],
   "source": [
    "cat_reduced = torch.zeros_like(cat_targs)\n",
    "pos=0\n",
    "for i, (k,v) in enumerate(total_cats.items()):\n",
    "    cat_reduced[:,i] = cat_preds[:,pos:pos+v].argmax(dim=1)\n",
    "    pos += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "zlH1IcACEdPH"
   },
   "outputs": [],
   "source": [
    "cat_preds = pd.DataFrame(cat_reduced, columns=cat_names)\n",
    "cat_targs = pd.DataFrame(cat_targs, columns=cat_names)\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "accuracy = pd.DataFrame.from_dict({c:[balanced_accuracy_score(cat_targs[c], cat_preds[c])] for c in cat_preds.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "ZMqj8zcKEdPJ"
   },
   "outputs": [],
   "source": [
    "f1 = pd.DataFrame.from_dict({c:[f1_score(cat_targs[c], cat_preds[c], average='weighted')] for c in cat_preds.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "vuBk02t5EdPL",
    "outputId": "432517e9-1e98-44c8-e82b-00dba4c73947"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MetricName</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>education-num_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MetricName  workclass  education  marital-status  occupation  relationship  \\\n",
       "0   Accuracy      0.650      0.818           0.714       0.755         0.842   \n",
       "0         F1      0.898      0.904           0.923       0.810         0.901   \n",
       "\n",
       "   race  education-num_na  \n",
       "0 0.648             0.867  \n",
       "0 0.931             0.994  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d,name in zip([accuracy, f1], ['Accuracy', 'F1']):\n",
    "    d = d.insert(0, 'MetricName', name)\n",
    "pd.concat([accuracy, f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GcWeUSqQXIG"
   },
   "source": [
    "And check it's overall accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "2rOtbaScEdPN",
    "outputId": "c685c93a-9a39-4d72-fa6c-f42fae03aa63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.756\n",
       "dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9K317948SvR"
   },
   "source": [
    "## Predicting\n",
    "\n",
    "Now that we have our compressed representations, let's use them to train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "HAM08-DsJsjh"
   },
   "outputs": [],
   "source": [
    "ys = df['salary'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "RRbccF9zKBV5"
   },
   "outputs": [],
   "source": [
    "test_eq(len(outs), len(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "OX0IPAG8KhbL"
   },
   "outputs": [],
   "source": [
    "df_outs = pd.DataFrame(columns=['salary'] + list(range(0,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "9KjjmrbfKqIN"
   },
   "outputs": [],
   "source": [
    "df_outs['salary'] = ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "gfO5CknNKtXl"
   },
   "outputs": [],
   "source": [
    "df_outs[list(range(0,128))] = outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "v6mUmbz5Lkqb"
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "l7lAP34pMAvN"
   },
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "FGezIL33MVXx"
   },
   "outputs": [],
   "source": [
    "df_outs[list(range(0,128))] = df_outs[list(range(0,128))].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "M1MWujHOKwBf"
   },
   "outputs": [],
   "source": [
    "cont_names = list(range(0,128))\n",
    "to = TabularPandas(df_outs, procs = [Normalize], cont_names=cont_names, splits=splits, y_names=['salary'], reduce_memory=False, \n",
    "                   y_block=CategoryBlock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "G4AKuKfbLgwQ"
   },
   "outputs": [],
   "source": [
    "dls = to.dataloaders(bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "NoYN1wi3NaYE"
   },
   "outputs": [],
   "source": [
    "def accuracy(inp, targ, axis=-1):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n",
    "    return (pred == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "lhOMRBYCLAZy"
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, layers=[200,100], metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "UuvlsOh4MlN_",
    "outputId": "64b00442-2801-4bfe-a9ca-d823c3140590"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.442242</td>\n",
       "      <td>0.435455</td>\n",
       "      <td>0.791769</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.410507</td>\n",
       "      <td>0.431449</td>\n",
       "      <td>0.791308</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.385457</td>\n",
       "      <td>0.444674</td>\n",
       "      <td>0.785166</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.358744</td>\n",
       "      <td>0.485525</td>\n",
       "      <td>0.765663</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.325381</td>\n",
       "      <td>0.568057</td>\n",
       "      <td>0.761824</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TabularAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
