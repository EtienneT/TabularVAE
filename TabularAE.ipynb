{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from functools import partial\n",
    "from pdb import set_trace\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.request\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_features.csv')\n",
    "df = pd.concat([df, pd.read_csv('data/test_features.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = ['cp_type', 'cp_dose']\n",
    "cont_names = df.columns[~df.columns.isin(cat_names)].tolist()\n",
    "cont_names.remove('sig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This simply reads a batch but instead of returning a dependant variable, just return the same thing as the input\n",
    "# The continuous variables are normalized though if we use the Normalize transform.  Didn't find an easy way of\n",
    "# outputing the non normalized continuous variables\n",
    "class ReadTabBatchIdentity(ItemTransform):\n",
    "    def __init__(self, to): self.to = to\n",
    "\n",
    "    def encodes(self, to):\n",
    "        if not to.with_cont: res = (tensor(to.cats).long(),) + (tensor(to.cats).long(),)\n",
    "        else: res = (tensor(to.cats).long(),tensor(to.conts).float()) + (tensor(to.cats).long(), tensor(to.conts).float())\n",
    "        if to.device is not None: res = to_device(res, to.device)\n",
    "        return res\n",
    "    \n",
    "class TabularPandasIdentity(TabularPandas): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates()\n",
    "class TabDataLoaderIdentity(TabDataLoader):\n",
    "    \"A transformed `DataLoader` for Tabular data\"\n",
    "    do_item = noops\n",
    "    def __init__(self, dataset, bs=16, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n",
    "        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTabBatchIdentity(dataset)\n",
    "        super(TabDataLoader, self).__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n",
    "\n",
    "    def create_batch(self, b): return self.dataset.iloc[b]\n",
    "\n",
    "TabularPandasIdentity._dl_type = TabDataLoaderIdentity\n",
    "\n",
    "to = TabularPandasIdentity(df, [Categorify, FillMissing, Normalize], cat_names, cont_names, splits=RandomSplitter(seed=32)(df))\n",
    "dls = to.dataloaders(bs=1024)\n",
    "dls.n_inp = 2\n",
    "\n",
    "emb_szs = get_emb_sz(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cp_type': 3, 'cp_dose': 3}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each categorical variable, we want to know how many possible values it has\n",
    "# We will need this in our loss function to figure out where to apply our F.cross_entropy for each categorical variables\n",
    "total_cats = {k:len(v) for k,v in dls.procs[1].classes.items()}\n",
    "total_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is going the be the # of outputs we are going to need for our categorical variables\n",
    "sum([v for k,v in total_cats.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = pd.DataFrame.from_dict({k:[v] for k,v in dls.procs[2].means.items()})\n",
    "stds = pd.DataFrame.from_dict({k:[v] for k,v in dls.procs[2].stds.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the life of our model easier, let's use a SigmoidRange to reduce the range of values that can be predicted for the continuous variables\n",
    "low = (df[cont_names].min().to_frame().T.values - means.values) / stds.values\n",
    "high = (df[cont_names].max().to_frame().T.values - means.values) / stds.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Swap Noise\n",
    "Used in the winning solution for the Kaggle competition [Puerto Seguro Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629#250927)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSwapNoise(nn.Module):\n",
    "    \"\"\"Swap Noise module\"\"\"\n",
    "\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = torch.rand(x.size()) > (1 - self.p)\n",
    "            l1 = torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor)\n",
    "            l2 = (mask.type(torch.LongTensor) * x.size(1))\n",
    "            res = (l1 * l2).view(-1)\n",
    "            idx = torch.arange(x.nelement()) + res\n",
    "            idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n",
    "            return x.flatten()[idx].view(x.size())\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularAE(TabularModel):\n",
    "    def __init__(self, emb_szs, n_cont, hidden_size, cats, low, high, ps=0.2, embed_p=0.01, bswap=None):\n",
    "        super().__init__(emb_szs, n_cont, layers=[1024, 512, 256], out_sz=hidden_size, embed_p=embed_p)\n",
    "        \n",
    "        self.bswap = bswap\n",
    "        self.cats = cats\n",
    "        self.activation_cats = sum([v for k,v in cats.items()])\n",
    "        \n",
    "        self.layers = nn.Sequential(*L(self.layers.children())[:-1] + nn.Sequential(LinBnDrop(256, hidden_size, p=ps, act=nn.ReLU(inplace=True))))\n",
    "        \n",
    "        if(bswap != None): self.noise = BatchSwapNoise(bswap)\n",
    "        self.decoder = nn.Sequential(\n",
    "            LinBnDrop(hidden_size, 256, p=ps, act=nn.ReLU(inplace=True)),\n",
    "            LinBnDrop(256, 512, p=ps, act=nn.ReLU(inplace=True)),\n",
    "            LinBnDrop(512, 1024, p=ps, act=nn.ReLU(inplace=True))\n",
    "        )\n",
    "        \n",
    "        self.decoder_cont = nn.Sequential(\n",
    "            LinBnDrop(1024, n_cont, p=ps, bn=False, act=None),\n",
    "            SigmoidRange(low=low, high=high)\n",
    "        )\n",
    "        \n",
    "        self.decoder_cat = LinBnDrop(1024, self.activation_cats, p=ps, bn=False, act=None)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont=None):\n",
    "        if(self.bswap != None):\n",
    "            x_cat = self.noise(x_cat)\n",
    "            x_cont = self.noise(x_cont)\n",
    "        \n",
    "        encoded = super().forward(x_cat, x_cont)\n",
    "        \n",
    "        decoded_trunk = self.decoder(encoded)\n",
    "        \n",
    "        decoded_cats = self.decoder_cat(decoded_trunk)\n",
    "        \n",
    "        decoded_conts = self.decoder_cont(decoded_trunk)\n",
    "        \n",
    "        return decoded_cats, decoded_conts\n",
    "\n",
    "def combined_loss(preds, cat_targs, cont_targs):\n",
    "    cats,conts = preds\n",
    "    \n",
    "    CE = cats.new([0])\n",
    "    pos=0\n",
    "    for i, (k,v) in enumerate(total_cats.items()):\n",
    "        CE += F.cross_entropy(cats[:, pos:pos+v], cat_targs[:, i], reduction='sum')\n",
    "        pos += v\n",
    "        \n",
    "    batch_size = cats.size(0)\n",
    "        \n",
    "    norm_cats = cats.new([len(total_cats.keys())])\n",
    "    norm_conts = conts.new([conts.size(1)])\n",
    "    total = (F.mse_loss(conts, cont_targs, reduction='sum')/norm_conts) + (CE/norm_cats)\n",
    "    \n",
    "    return total / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test that everything works for a batch...\n",
    "model = TabularAE(emb_szs, len(cont_names), 128, total_cats, low=tensor(low).cuda(), high=tensor(high).cuda()).cuda()\n",
    "r = model(out[0].cuda(), out[1].cuda())\n",
    "loss = combined_loss(r, out[2], out[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.callback.tracker import *\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "class WandbCallbackCustom(WandbCallback):\n",
    "    def log_predictions(self, preds):\n",
    "        inp,(cat_preds, cont_preds),(cat_targs, cont_targs),out = preds\n",
    "\n",
    "        cont_preds = pd.DataFrame(cont_preds, columns=cont_names)\n",
    "        cont_targs = pd.DataFrame(cont_targs, columns=cont_names)\n",
    "\n",
    "        preds = pd.DataFrame((cont_preds.values * stds.values) + means.values, columns=cont_preds.columns)\n",
    "        targets = pd.DataFrame((cont_targs.values * stds.values) + means.values, columns=cont_targs.columns)\n",
    "\n",
    "        mi = (np.abs(targets-preds)).min().to_frame().T\n",
    "        ma = (np.abs(targets-preds)).max().to_frame().T\n",
    "        mean = (np.abs(targets-preds)).mean().to_frame().T\n",
    "        median = (np.abs(targets-preds)).median().to_frame().T\n",
    "        r2 = pd.DataFrame.from_dict({c:[r2_score(targets[c], preds[c])] for c in preds.columns})\n",
    "\n",
    "        for d,name in zip([mi,ma,mean,median,r2], ['Min', 'Max', 'Mean', 'Median', 'R2']):\n",
    "            d = d.insert(0, 'GroupBy', name)\n",
    "\n",
    "        data = pd.concat([r2,mi,ma,mean,median])\n",
    "        \n",
    "        cat_reduced = torch.zeros_like(cat_targs)\n",
    "        pos=0\n",
    "        for i, (k,v) in enumerate(total_cats.items()):\n",
    "            cat_reduced[:,i] = cat_preds[:,pos:pos+v].argmax(dim=1)\n",
    "            pos += v\n",
    "        \n",
    "        cat_preds = pd.DataFrame(cat_reduced, columns=cat_names)\n",
    "        cat_targs = pd.DataFrame(cat_targs, columns=cat_names)\n",
    "\n",
    "        accuracy = pd.DataFrame.from_dict({c:[balanced_accuracy_score(cat_targs[c], cat_preds[c])] for c in cat_preds.columns})\n",
    "        f1 = pd.DataFrame.from_dict({c:[f1_score(cat_targs[c], cat_preds[c], average='weighted')] for c in cat_preds.columns})\n",
    "        \n",
    "        tolog = {}\n",
    "        \n",
    "#         for c in preds.columns:\n",
    "#             tolog[c + '_MAE'] = wandb.Histogram(np.abs(preds[c]-targets[c]))\n",
    "            \n",
    "        for c in preds.columns:\n",
    "            tolog[c + '_R2'] = r2[c][0]\n",
    "            \n",
    "        for c in accuracy.columns:\n",
    "            tolog[c + '_Accuracy'] = accuracy[c][0]\n",
    "            \n",
    "        for c in accuracy.columns:\n",
    "            tolog[c + '_F1'] = f1[c][0]\n",
    "        \n",
    "        for d,name in zip([accuracy, f1], ['Accuracy', 'F1']):\n",
    "            d = d.insert(0, 'MetricName', name)\n",
    "            \n",
    "        tolog['MeanR2'] = r2.mean(axis=1)[0]\n",
    "        tolog['StdR2'] = r2.std(axis=1)[0]\n",
    "        tolog['MeanAccuracy'] = accuracy.mean(axis=1)[0]\n",
    "        tolog['MeanF1'] = f1.mean(axis=1)[0]\n",
    "        \n",
    "        tolog['continuous'] = wandb.Table(dataframe=data)\n",
    "        tolog['categorical'] = wandb.Table(dataframe=pd.concat([accuracy, f1]))\n",
    "        \n",
    "        wandb.log(tolog, step=self._wandb_step)\n",
    "        \n",
    "config = {\n",
    "    'hidden_size': 128,\n",
    "    'dropout': 0.1,\n",
    "    'embed_p': 0.01,\n",
    "    'wd': 0.01,\n",
    "    'bswap': 0.1,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "cbs = [SaveModelCallback(fname='tabular' + datetime.now().strftime('%Y-%m-%d %Hh%M.%S'), every_epoch=True)]\n",
    "\n",
    "# wandb.init(project='Kaggle-Lish-Moa', config=config)\n",
    "# cbs += [WandbCallbackCustom(log_model=False)]\n",
    "\n",
    "model = TabularAE(emb_szs, len(cont_names), config['hidden_size'], ps=config['dropout'], cats=total_cats, embed_p=config['embed_p'], bswap=config['bswap'], low=tensor(low).cuda(), high=tensor(high).cuda())\n",
    "learn = Learner(dls, model, lr=config['lr'], loss_func=combined_loss, wd=config['wd'], cbs=cbs)\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.797834</td>\n",
       "      <td>3.888723</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.906911</td>\n",
       "      <td>2.617576</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.122289</td>\n",
       "      <td>1.716328</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.482240</td>\n",
       "      <td>1.317152</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.993878</td>\n",
       "      <td>0.974741</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.619559</td>\n",
       "      <td>0.776361</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.340700</td>\n",
       "      <td>0.683106</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.142212</td>\n",
       "      <td>0.637574</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.998390</td>\n",
       "      <td>0.605221</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.895347</td>\n",
       "      <td>0.589779</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.817349</td>\n",
       "      <td>0.567558</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.759184</td>\n",
       "      <td>0.550004</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.714971</td>\n",
       "      <td>0.542629</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.678194</td>\n",
       "      <td>0.529341</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.649128</td>\n",
       "      <td>0.523985</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.625685</td>\n",
       "      <td>0.528145</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.606280</td>\n",
       "      <td>0.512272</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.590039</td>\n",
       "      <td>0.503362</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.577442</td>\n",
       "      <td>0.491930</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.568142</td>\n",
       "      <td>0.492417</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.558370</td>\n",
       "      <td>0.497866</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.550884</td>\n",
       "      <td>0.475363</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.541077</td>\n",
       "      <td>0.476119</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.533587</td>\n",
       "      <td>0.480524</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.525696</td>\n",
       "      <td>0.470499</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.518899</td>\n",
       "      <td>0.466129</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.512375</td>\n",
       "      <td>0.467810</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.509023</td>\n",
       "      <td>0.469894</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.506862</td>\n",
       "      <td>0.474767</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.501731</td>\n",
       "      <td>0.463120</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.498734</td>\n",
       "      <td>0.459134</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.492633</td>\n",
       "      <td>0.455241</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.489575</td>\n",
       "      <td>0.452620</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.486073</td>\n",
       "      <td>0.447008</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.483102</td>\n",
       "      <td>0.449626</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.479662</td>\n",
       "      <td>0.454428</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.477411</td>\n",
       "      <td>0.464678</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.474172</td>\n",
       "      <td>0.447746</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.471518</td>\n",
       "      <td>0.453057</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.468209</td>\n",
       "      <td>0.444408</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.465942</td>\n",
       "      <td>0.453896</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.462836</td>\n",
       "      <td>0.443945</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.459351</td>\n",
       "      <td>0.436468</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.456956</td>\n",
       "      <td>0.448404</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.454992</td>\n",
       "      <td>0.433425</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.452502</td>\n",
       "      <td>0.429687</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.449988</td>\n",
       "      <td>0.434134</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.447727</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.445737</td>\n",
       "      <td>0.427410</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.443005</td>\n",
       "      <td>0.425297</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.440391</td>\n",
       "      <td>0.435134</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.439622</td>\n",
       "      <td>0.426669</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.437583</td>\n",
       "      <td>0.421161</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.436121</td>\n",
       "      <td>0.427111</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.433891</td>\n",
       "      <td>0.424223</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.420976</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.430549</td>\n",
       "      <td>0.423876</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.429314</td>\n",
       "      <td>0.421659</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.427366</td>\n",
       "      <td>0.424393</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.425499</td>\n",
       "      <td>0.419025</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.424575</td>\n",
       "      <td>0.420754</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.422638</td>\n",
       "      <td>0.418133</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.421165</td>\n",
       "      <td>0.417907</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.419556</td>\n",
       "      <td>0.422407</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.425644</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.418124</td>\n",
       "      <td>0.418673</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.416708</td>\n",
       "      <td>0.421090</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.416390</td>\n",
       "      <td>0.420024</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.414548</td>\n",
       "      <td>0.417845</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.413090</td>\n",
       "      <td>0.416896</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.412560</td>\n",
       "      <td>0.423282</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.411592</td>\n",
       "      <td>0.419316</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.409710</td>\n",
       "      <td>0.412935</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.408374</td>\n",
       "      <td>0.420645</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.407695</td>\n",
       "      <td>0.417020</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.406634</td>\n",
       "      <td>0.418525</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.405954</td>\n",
       "      <td>0.427002</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.405072</td>\n",
       "      <td>0.418868</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.403948</td>\n",
       "      <td>0.414849</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.403147</td>\n",
       "      <td>0.417439</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.402557</td>\n",
       "      <td>0.419344</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.401492</td>\n",
       "      <td>0.418494</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.400674</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.400350</td>\n",
       "      <td>0.418997</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.416354</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.399068</td>\n",
       "      <td>0.420367</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.398698</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.398240</td>\n",
       "      <td>0.416525</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.397804</td>\n",
       "      <td>0.418837</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.397660</td>\n",
       "      <td>0.419715</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.397506</td>\n",
       "      <td>0.419580</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.396996</td>\n",
       "      <td>0.418210</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.396785</td>\n",
       "      <td>0.421247</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.396705</td>\n",
       "      <td>0.420005</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.396032</td>\n",
       "      <td>0.420670</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.395737</td>\n",
       "      <td>0.421682</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.395701</td>\n",
       "      <td>0.420153</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.395367</td>\n",
       "      <td>0.421334</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.395683</td>\n",
       "      <td>0.421249</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.395587</td>\n",
       "      <td>0.420353</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(config['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the compressed representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = learn.dls.test_dl(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBottleNeckCallback(HookCallback):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.preds = []\n",
    "        \n",
    "    def hook(self, m, i, o):\n",
    "        self.preds += [self.learn.to_detach(i[0])]\n",
    "    \n",
    "# model.decoder[0][0] is the first layer of the decoder layer, we are saving the input of this layer since this is our compressed representations\n",
    "hook = GetBottleNeckCallback(modules=[learn.model.decoder[0][0]])\n",
    "learn.add_cbs(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(cat_preds, cont_preds), (cat_targs, cont_targs) = learn.get_preds(dl=dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27796, 128)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed = np.concatenate(hook.preds)\n",
    "compressed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final reconstruction stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroupBy</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Min</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Max</td>\n",
       "      <td>47.813</td>\n",
       "      <td>5.605</td>\n",
       "      <td>4.860</td>\n",
       "      <td>6.485</td>\n",
       "      <td>9.311</td>\n",
       "      <td>5.905</td>\n",
       "      <td>9.628</td>\n",
       "      <td>9.462</td>\n",
       "      <td>5.863</td>\n",
       "      <td>...</td>\n",
       "      <td>4.441</td>\n",
       "      <td>6.747</td>\n",
       "      <td>4.914</td>\n",
       "      <td>5.854</td>\n",
       "      <td>6.631</td>\n",
       "      <td>6.040</td>\n",
       "      <td>5.947</td>\n",
       "      <td>5.151</td>\n",
       "      <td>6.200</td>\n",
       "      <td>6.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean</td>\n",
       "      <td>9.412</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Median</td>\n",
       "      <td>8.262</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 874 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  GroupBy  cp_time   g-0   g-1   g-2   g-3   g-4   g-5   g-6   g-7  ...  c-90  \\\n",
       "0     Min    0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "0     Max   47.813 5.605 4.860 6.485 9.311 5.905 9.628 9.462 5.863  ... 4.441   \n",
       "0    Mean    9.412 0.387 0.477 0.670 0.448 0.412 0.693 0.369 0.349  ... 0.468   \n",
       "0  Median    8.262 0.299 0.389 0.528 0.345 0.274 0.504 0.295 0.258  ... 0.374   \n",
       "0      R2    0.657 0.856 0.411 0.247 0.541 0.617 0.310 0.673 0.806  ... 0.905   \n",
       "\n",
       "   c-91  c-92  c-93  c-94  c-95  c-96  c-97  c-98  c-99  \n",
       "0 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "0 6.747 4.914 5.854 6.631 6.040 5.947 5.151 6.200 6.443  \n",
       "0 0.480 0.512 0.482 0.441 0.522 0.483 0.490 0.522 0.503  \n",
       "0 0.379 0.407 0.381 0.346 0.420 0.377 0.402 0.399 0.416  \n",
       "0 0.897 0.885 0.905 0.923 0.823 0.900 0.861 0.846 0.788  \n",
       "\n",
       "[5 rows x 874 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "cont_preds = pd.DataFrame(cont_preds, columns=cont_names)\n",
    "cont_targs = pd.DataFrame(cont_targs, columns=cont_names)\n",
    "\n",
    "preds = pd.DataFrame((cont_preds.values * stds.values) + means.values, columns=cont_preds.columns)\n",
    "targets = pd.DataFrame((cont_targs.values * stds.values) + means.values, columns=cont_targs.columns)\n",
    "\n",
    "mi = (np.abs(targets-preds)).min().to_frame().T\n",
    "ma = (np.abs(targets-preds)).max().to_frame().T\n",
    "mean = (np.abs(targets-preds)).mean().to_frame().T\n",
    "median = (np.abs(targets-preds)).median().to_frame().T\n",
    "r2 = pd.DataFrame.from_dict({c:[r2_score(targets[c], preds[c])] for c in preds.columns})\n",
    "\n",
    "\n",
    "for d,name in zip([mi,ma,mean,median,r2], ['Min', 'Max', 'Mean', 'Median', 'R2']):\n",
    "    d = d.insert(0, 'GroupBy', name)\n",
    "    \n",
    "data = pd.concat([mi,ma,mean,median,r2])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.635\n",
       "dtype: float64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reduced = torch.zeros_like(cat_targs)\n",
    "pos=0\n",
    "for i, (k,v) in enumerate(total_cats.items()):\n",
    "    cat_reduced[:,i] = cat_preds[:,pos:pos+v].argmax(dim=1)\n",
    "    pos += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preds = pd.DataFrame(cat_reduced, columns=cat_names)\n",
    "cat_targs = pd.DataFrame(cat_targs, columns=cat_names)\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "accuracy = pd.DataFrame.from_dict({c:[balanced_accuracy_score(cat_targs[c], cat_preds[c])] for c in cat_preds.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.DataFrame.from_dict({c:[f1_score(cat_targs[c], cat_preds[c], average='weighted')] for c in cat_preds.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MetricName</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_dose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MetricName  cp_type  cp_dose\n",
       "0   Accuracy    0.994    1.000\n",
       "0         F1    0.997    1.000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d,name in zip([accuracy, f1], ['Accuracy', 'F1']):\n",
    "    d = d.insert(0, 'MetricName', name)\n",
    "pd.concat([accuracy, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   0.997\n",
       "dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
